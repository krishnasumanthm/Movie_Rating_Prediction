{
 "metadata": {
  "name": "",
  "signature": "sha256:4d3ddcd83a924bc8a66127cc4917bc34472da288697f8420e3d71f346572f332"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Importing necessary packages\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "import os\n",
      "import replacers\n",
      "import numpy as np\n",
      "import pickle\n",
      "import time\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function for reading stopwords from a file\n",
      "def readStopwordsandStoreinList(filename):\n",
      "    my_file=open(filename)\n",
      "    stopwordList = []\n",
      "    for line in my_file:\n",
      "        if (line!=\"\\n\"): \n",
      "            line=line.strip()\n",
      "            stopwordList.append(line)\n",
      "    return stopwordList\n",
      "\n",
      "stop = readStopwordsandStoreinList(\"stopwordsTemp.txt\")\n",
      "stop = nltk.corpus.stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function for preprocessing the text\n",
      "def common_preprocessing(text):\n",
      "    '''\n",
      "    Includes common preprocessing tasks on text data -lowercasing, term expansion, spelling correction, repeated chars removal\n",
      "    '''\n",
      "    global stop\n",
      "    \n",
      "    # Converting to lowercase\n",
      "    text = text.lower()\n",
      "    \n",
      "    # Term Expansion, eg. won't -> will not, we've -> we have, etc\n",
      "    term_expander = replacers.RegexpReplacer()\n",
      "    text = term_expander.replace(text)\n",
      "    tokens = text.split()\n",
      "    \n",
      "    # Repeated character removal, eg. loooovvveee -> love\n",
      "    repeated_char_remover = replacers.RepeatReplacer()\n",
      "    tokens = [repeated_char_remover.replace(token) for token in tokens]\n",
      "    \n",
      "    # Spelling correction within one edit distance of dictionary word\n",
      "    spelling_corrector\t= replacers.SpellingReplacer()\n",
      "    tokens\t= [spelling_corrector.replace(token) for token in tokens]\n",
      "    \n",
      "    # Stopwords Removal\n",
      "    tokens = [i for i in tokens if i.lower() not in stop]\n",
      "    text = ' '.join(tokens)\n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function for building the training dataset\n",
      "def build_train_set():\n",
      "    data = []\n",
      "    label = []\n",
      "    filelist1 = []\n",
      "    filelist2 = []\n",
      "    global stop\n",
      "    count1 = 1\n",
      "    count2 = 1\n",
      "    for root,dirs,files in os.walk('train_data/pos'):\n",
      "        filelist1.extend(files)\n",
      "        break\n",
      "    for files in filelist1:\n",
      "        try:\n",
      "            f=open('train_data/pos/' + files, 'r')\n",
      "            for line in f:\n",
      "                data.append(common_preprocessing(line))\n",
      "                label.append('1')\n",
      "                #print count1\n",
      "                #count1 = count1+1\n",
      "        except:\n",
      "            continue\n",
      "    print len(data)           \n",
      "    \n",
      "    for root,dirs,files in os.walk('train_data/neg'):\n",
      "        filelist2.extend(files)\n",
      "        break\n",
      "    for files in filelist2:\n",
      "        try:\n",
      "            f=open('train_data/neg/' + files, 'r')\n",
      "            for line in f:\n",
      "                data.append(common_preprocessing(line))\n",
      "                label.append('-1')\n",
      "                #print count2\n",
      "                #count2 = count2+1\n",
      "        except:\n",
      "            continue\n",
      "    print len(data)           \n",
      "    \n",
      "    return data,label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Getting the train data and train labels\n",
      "start_time = time.time()\n",
      "train_data,train_labels = build_train_set()\n",
      "print time.time() - start_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "12500\n",
        "25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "130.447334051\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Converting the train train labels into array format for scikit-learn operations\n",
      "start_time = time.time()\n",
      "X_train = np.array([''.join(el) for el in train_data[0:len(train_data)]])\n",
      "y_train = np.array([el for el in train_labels[0:len(train_labels)]])\n",
      "print time.time() - start_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.689619064331\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Defining a vectorizer for feature extraction\n",
      "start_time = time.time()\n",
      "vectorizer = CountVectorizer(min_df=1,ngram_range=(1, 2))\n",
      "print time.time() - start_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.739148139954\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Extracting features from train data\n",
      "start_time = time.time()\n",
      "X_train = vectorizer.fit_transform(X_train) \n",
      "print time.time() - start_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "28.4150400162\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Defing a classifier for classification\n",
      "start_time = time.time()\n",
      "classifier = LinearSVC().fit(X_train, y_train)\n",
      "clf = open('classifier.pickle', 'wb')\n",
      "vec = open('vectorizer.pickle','wb')\n",
      "#Pickle saves the classifier and vectorizer which can be used for test data\n",
      "pickle.dump(classifier, clf)\n",
      "pickle.dump(vectorizer,vec)\n",
      "clf.close()\n",
      "vec.close()\n",
      "print time.time() - start_time\n",
      "print 'Training Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "125.216298819\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}